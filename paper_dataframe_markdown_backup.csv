Theme,Number,Title,Journal/Conference,Date,Author,Link,,,
"<a id=""neural-network"">Neural Network</a><br />",1,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,JMLR 2014,2014-01-01,Nitish Srivastava et al,[Link](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf),,,
,2,Convolutional Neural Networks for Sentence Classification,EMNLP? 2014,2014-08-25,Yoon Kim et al,[Link](http://arxiv.org/abs/1408.5882v2),,,
,3,Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation,,2016-07-01,Dong Yu et al,[Link](http://arxiv.org/abs/1607.00325v2),,,
,4,Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour,NeurIPS 2017,2017-06-08,Priya Goyal et al,[Link](http://arxiv.org/abs/1706.02677v2),,
,5,Don't Decay the Learning Rate, Increase the Batch Size,ICLR? 2018,2017-11-01,Samuel L. Smith et al,[Link](http://arxiv.org/abs/1711.00489v2),,
,6,A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay,PBML,2018-03-26,Leslie N. Smith et al,[Link](http://arxiv.org/abs/1803.09820v2)
,7,Dying ReLU and Initialization: Theory and Numerical Examples,,2019-03-15,Lu Lu et al,[Link](http://arxiv.org/abs/1903.06733v3),,,
,8,Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks,PMLR 2021,2020-10-29,Julieta Martinez et al,[Link](http://arxiv.org/abs/2010.15703v3),
"<a id=""#benchmark"">Benchmark</a>",9,BLEU: a method for automatic evaluation of machine translation,ACL 2002,2002-07-01,Kishore Papineni et al,[Link](https://dl.acm.org/doi/10.3115/1073083.1073135),,,
,10,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,,2018-04-20,Alex Wang et al,[Link](http://arxiv.org/abs/1804.07461v3),,,
,11,ROUGE: A Package for Automatic Evaluation of Summaries,Artificial Intelligenc Volume 299,2021-10-01,Chin-Yew Lin et al,[Link](https://aclanthology.org/W04-1013/),,,
"<a id=""#word-embedding"">Word Embedding</a>",12,Efficient Estimation of Word Representations in Vector Space,,2013-01-16,Tomas Mikolov et al,[Link](http://arxiv.org/abs/1301.3781v3),,,
,13,Linguistic Regularities in Continuous Space Word Representations,,2013-06-01,Tomas Mikolov et al,[Link](https://aclanthology.org/N13-1090/),,,
,14,Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer,,2018-03-17,Sudha Rao et al,[Link](http://arxiv.org/abs/1803.06535v2),
,15,SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,ACL 2019,2018-08-19,Taku Kudo et al,[Link](http://arxiv.org/abs/1808.06226v1),,,
,16,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,AAAI 2020,2019-08-27,Nils Reimers et al,[Link](http://arxiv.org/abs/1908.10084v1),,,
,17,SimCSE: Simple Contrastive Learning of Sentence Embeddings,EMNLP? 2021,2021-04-18,Tianyu Gao et al,[Link](http://arxiv.org/abs/2104.08821v4),,,
,18,Self-Guided Contrastive Learning for BERT Sentence Representations,ACL? 2021,2021-06-03,Taeuk Kim et al,[Link](http://arxiv.org/abs/2106.07345v1),,,
,19,Contrastive Learning of Sentence Embeddings from Scratch,,2023-05-24,Junlei Zhang et al,[Link](http://arxiv.org/abs/2305.15077v2),,,
"<a id=""#explainable-artificial-intelligence"">Explainable Artificial Intelligence (XAI)</a>",20,Building Machines That Learn and Think Like People,NeurIPS 2016,2016-04-01,Brenden M. Lake et al,[Link](http://arxiv.org/abs/1604.00289v3),,,
,21,A Multiscale Visualization of Attention in the Transformer Model,ACL? 2019,2019-06-12,Jesse Vig et al,[Link](http://arxiv.org/abs/1906.05714v1),,,
,22,On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines,IEEE Open Journal of the Computer Society,2020-06-08,Marius Mosbach et al,[Link](http://arxiv.org/abs/2006.04884v3),
,23,Reliable Post hoc Explanations: Modeling Uncertainty in Explainability,,2020-08-11,Dylan Slack et al,[Link](http://arxiv.org/abs/2008.05030v4),,,
,24,Reward is enough,ICLR 2022,2021-10-01,David Silver et al,[Link](https://www.sciencedirect.com/science/article/pii/S0004370221000862),,,
"<a id=""#language-model"">Language Model</a>",25,Large-Scale Distributed Language Modeling,IEEE 2007,2007-04-05,Ahmad Emami et al,[Link](https://ieeexplore.ieee.org/document/4218031),,,
,26,Large Language Models in Machine Translation,Teaching and Learning in Higher Education,2007-06-01,Gloria Brown Wright et al,[Link](https://aclanthology.org/D07-1090/),,,
,27,Neural Machine Translation by Jointly Learning to Align and Translate,NeurIPS 2014,2014-09-01,Dzmitry Bahdanau et al,[Link](http://arxiv.org/abs/1409.0473v7),,,
,28,Sequence to Sequence Learning with Neural Networks,ICML 2015,2014-09-10,Ilya Sutskever et al,[Link](http://arxiv.org/abs/1409.3215v3),,,
,29,Neural Machine Translation of Rare Words with Subword Units,ICLR 2016,2015-08-31,Rico Sennrich et al,[Link](http://arxiv.org/abs/1508.07909v5),,,
,30,Continuous control with deep reinforcement learning,,2015-09-09,Timothy P. Lillicrap et al,[Link](http://arxiv.org/abs/1509.02971v6),,,
,31,Hierarchical Attention Networks for Document Classification,,2016-01-01,Zichao Yang et al,[Link](https://aclanthology.org/N16-1174/),,,
,32,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,,2016-09-26,Yonghui Wu et al,[Link](http://arxiv.org/abs/1609.08144v2),,,
,33,Learning to Ask: Neural Question Generation for Reading Comprehension,ACL? 2017,2017-04-29,Xinya Du et al,[Link](http://arxiv.org/abs/1705.00106v1),,,
,34,Style Transfer from Non-Parallel Text by Cross-Alignment,,2017-05-26,Tianxiao Shen et al,[Link](http://arxiv.org/abs/1705.09655v2),,,
,35,Attention Is All You Need,,2017-06-12,Ashish Vaswani et al,[Link](http://arxiv.org/abs/1706.03762v5),,,
,36,Adversarial Examples for Evaluating Reading Comprehension Systems,,2017-07-23,Robin Jia et al,[Link](http://arxiv.org/abs/1707.07328v1),,,
,37,Deep contextualized word representations,,2018-02-15,Matthew E. Peters et al,[Link](http://arxiv.org/abs/1802.05365v2),,,
,38,Training Tips for the Transformer Model,,2018-04-01,Martin Popel et al,[Link](http://arxiv.org/abs/1804.00247v2),,,
,39,Know What You Don't Know: Unanswerable Questions for SQuAD,ACL? 2018,2018-06-11,Pranav Rajpurkar et al,[Link](http://arxiv.org/abs/1806.03822v1),,,
,40,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,,2018-10-11,Jacob Devlin et al,[Link](http://arxiv.org/abs/1810.04805v2),,,
,41,Language Models are Unsupervised Multitask Learners,,2019-06-01,Alec Radford et al,[Link](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe),,,
,42,SpanBERT: Improving Pre-training by Representing and Predicting Spans,,2019-07-24,Mandar Joshi et al,[Link](http://arxiv.org/abs/1907.10529v3),,,
,43,Alpaca: Intermittent Execution without Checkpoints,NeurIPS 2019,2019-09-13,Kiwan Maeng et al,[Link](http://arxiv.org/abs/1909.06951v1),,,
,44,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,ACL? 2020,2019-10-23,Colin Raffel et al,[Link](http://arxiv.org/abs/1910.10683v4),,,
,45,BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension,,2019-10-29,Mike Lewis et al,[Link](http://arxiv.org/abs/1910.13461v1),
,46,Improving Transformer Optimization Through Better Initialization,NeurIPS 2020,2020-01-01,Xiao Shi Huang et al,[Link](https://proceedings.mlr.press/v119/huang20f.html),,,
,47,Understanding the Difficulty of Training Transformers,,2020-04-17,Liyuan Liu et al,[Link](http://arxiv.org/abs/2004.08249v3),,,
,48,Finetuned Language Models Are Zero-Shot Learners,IEEE/RJS International Conference on Intelligent RObots and Systems,2021-09-03,Jason Wei et al,[Link](http://arxiv.org/abs/2109.01652v5),,,
,49,Post-Training with Interrogative Sentences for Enhancing BART-based Korean Question Generator,ACL 2022,2022-01-01,Gyu-Min Park et al,[Link](https://aclanthology.org/2022.aacl-short.26/),,,
,50,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,EMNLP? 2022,2022-04-16,Yizhong Wang et al,[Link](http://arxiv.org/abs/2204.07705v3),,,
,51,Generative Language Models for Paragraph-Level Question Generation,EMNLP? 2022,2022-10-08,Asahi Ushio et al,[Link](http://arxiv.org/abs/2210.03992v3),,,
,52,NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers,CVPR 2023,2022-11-29,Yijiang Liu et al,[Link](http://arxiv.org/abs/2211.16056v2),,,
,53,Self-Instruct: Aligning Language Models with Self-Generated Instructions,ACL? 2023,2022-12-20,Yizhong Wang et al,[Link](http://arxiv.org/abs/2212.10560v2),,,
,54,Dialog-Post Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training,ACL 2023,2023-01-01,Zhenyu Zhang et al,[Link](https://aclanthology.org/2023.acl-long.564/),,,
,55,LLaMA: Open and Efficient Foundation Language Models,,2023-02-27,Hugo Touvron et al,[Link](http://arxiv.org/abs/2302.13971v1),,,
,56,Instruction Tuning with GPT-4,EMNLP 2023,2023-04-06,Baolin Peng et al,[Link](http://arxiv.org/abs/2304.03277v1),,,
,57,An Empirical Comparison of LM-based Question and Answer Generation Methods,ACL? 2023,2023-05-26,Asahi Ushio et al,[Link](http://arxiv.org/abs/2305.17002v1),,,
,58,A Practical Toolkit for Multilingual Question and Answer Generation,,2023-05-27,Asahi Ushio et al,[Link](http://arxiv.org/abs/2305.17416v1),,,
"<a id=""#meta-learning"">Meta Learning</a>",59,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,ICML? 2017,2017-03-09,Chelsea Finn et al,[Link](http://arxiv.org/abs/1703.03400v3),,,
,60,Optimization as A Model for Few-shot Learning,ICLR 2017,2017-07-22,Sachin Ravi et al,[Link](https://openreview.net/forum?id=rJY0-Kcll),,,
,61,BERT Learns to Teach: Knowledge Distillation with Meta Learning,,2021-06-08,Wangchunshu Zhou et al,[Link](http://arxiv.org/abs/2106.04570v3),,,
"<a id=""#continual-learning"">Continual Learning</a>",62,Overcoming catastrophic forgetting in neural networks,,2016-12-02,James Kirkpatrick et al,[Link](http://arxiv.org/abs/1612.00796v2),,,
"<a id=""#mixture-of-experts"">Mixture of Experts</a>",63,Adaptive Mixtures of Local Experts,MIT Press 1991,1991-03-01,Robert A. Jacobs et al,[Link](https://ieeexplore.ieee.org/abstract/document/6797059),,,
"<a id=""#ensemble"">Ensemble</a>",64,Ensemble deep learning: A review,,2021-04-06,M. A. Ganaie et al,[Link](http://arxiv.org/abs/2104.02395v3),,,
"<a id=""#model-compression"">Model Compression</a>",65,Model Compression,ACM SIGKDD 2006,2006-08-20,Cristian Bucil¢¨a et al,[Link](https://dl.acm.org/doi/abs/10.1145/1150402.1150464),,,
,66,Adaptive Computation Time for Recurrent Neural Networks,Behavioral and Brain Sciences,2016-03-29,Alex Graves et al,[Link](http://arxiv.org/abs/1603.08983v6),,,
,67,BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks,,2017-09-06,Surat Teerapittayanon et al,[Link](http://arxiv.org/abs/1709.01686v1),,,
,68,FastBERT: a Self-distilling BERT with Adaptive Inference Time,PMLR 2020,2020-04-05,Weijie Liu et al,[Link](http://arxiv.org/abs/2004.02178v2),,,
,69,A Survey on Model Compression and Acceleration for Pretrained Language Models,ICLR 2022,2022-02-15,Canwen Xu et al,[Link](http://arxiv.org/abs/2202.07105v2),,,
"<a id=""#knoweldge-distillation"">Knoweldge Distillation</a>",70,Distilling the Knowledge in a Neural Network,NIPS? 2014,2015-03-09,Geoffrey Hinton et al,[Link](http://arxiv.org/abs/1503.02531v1),,,
,71,Improved Knowledge Distillation via Teacher Assistant,ACSAC 2019,2019-02-09,Seyed-Iman Mirzadeh et al,[Link](http://arxiv.org/abs/1902.03393v2),,,
,72,Unified Language Model Pre-training for Natural Language Understanding and Generation,,2019-05-08,Li Dong et al,[Link](http://arxiv.org/abs/1905.03197v3),,,
,73,Patient Knowledge Distillation for BERT Model Compression,EMNLP? 2019,2019-08-25,Siqi Sun et al,[Link](http://arxiv.org/abs/1908.09355v1),,,
,74,DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,JMLR,2019-10-02,Victor Sanh et al,[Link](http://arxiv.org/abs/1910.01108v4)
,75,MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,,2020-02-25,Wenhui Wang et al,[Link](http://arxiv.org/abs/2002.10957v2),,,
,76,MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers,PMLR 2021,2020-12-31,Wenhui Wang et al,[Link](http://arxiv.org/abs/2012.15828v2),,,
"<a id=""#quantization"">Quantization</a>",77,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation,NeurIPS 2013,2013-08-15,Yoshua Bengio et al,[Link](http://arxiv.org/abs/1308.3432v1),,,
,78,XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,,2016-03-16,Mohammad Rastegari et al,[Link](http://arxiv.org/abs/1603.05279v4),,,
,79,DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients,IEEE 2017,2016-06-20,Shuchang Zhou et al,[Link](http://arxiv.org/abs/1606.06160v3),,,
,80,Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations,,2016-09-22,Itay Hubara et al,[Link](http://arxiv.org/abs/1609.07061v1),,,
,81,Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,,2017-12-15,Benoit Jacob et al,[Link](http://arxiv.org/abs/1712.05877v1),,,
,82,HAQ: Hardware-Aware Automated Quantization with Mixed Precision,CVPR? 2019,2018-11-21,Kuan Wang et al,[Link](http://arxiv.org/abs/1811.08886v3),,,
,83,And the Bit Goes Down: Revisiting the Quantization of Neural Networks,TACL 2020,2019-07-12,Pierre Stock et al,[Link](http://arxiv.org/abs/1907.05686v5),,,
,84,Learned Step Size Quantization,ICLR 2020,2019-02-21,Steven K. Esser et al,[Link](http://arxiv.org/abs/1902.08153v3),,,
,85,Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT,,2019-09-12,Sheng Shen et al,[Link](http://arxiv.org/abs/1909.05840v2),,,
,86,Quantization Networks,,2019-11-21,Jiwei Yang et al,[Link](http://arxiv.org/abs/1911.09464v2),,,
,87,ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions,,2020-03-07,Zechun Liu et al,[Link](http://arxiv.org/abs/2003.03488v2),,,
,88,Binary Neural Networks: A Survey,,2020-03-31,Haotong Qin et al,[Link](http://arxiv.org/abs/2004.03333v1),,,
,89,Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation,ACSAC 2021,2020-04-20,Hao Wu et al,[Link](http://arxiv.org/abs/2004.09602v1),,,
,90,BinaryBERT: Pushing the Limit of BERT Quantization,ACL-IJCNLP 2021,2020-12-31,Haoli Bai et al,[Link](http://arxiv.org/abs/2012.15701v2),,,
,91,I-BERT: Integer-only BERT Quantization,USENIX 2021,2021-01-05,Sehoon Kim et al,[Link](http://arxiv.org/abs/2101.01321v3),,,
,92,A Survey of Quantization Methods for Efficient Neural Network Inference,AAAI 2022,2021-03-25,Amir Gholami et al,[Link](http://arxiv.org/abs/2103.13630v3),,,
,93,A White Paper on Neural Network Quantization,ICLR 2022,2021-06-15,Markus Nagel et al,[Link](http://arxiv.org/abs/2106.08295v1),,,
,94,BiBERT: Accurate Fully Binarized BERT,,2022-03-12,Haotong Qin et al,[Link](http://arxiv.org/abs/2203.06390v1),,,
,95,BiT: Robustly Binarized Multi-distilled Transformer,IPS? 2022,2022-05-25,Zechun Liu et al,[Link](http://arxiv.org/abs/2205.13016v2),,,
"<a id=""#reinforcement-learning"">Reinforcement Learning</a>",96,Playing Atari with Deep Reinforcement Learning,ICLR 2014,2013-12-19,Volodymyr Mnih et al,[Link](http://arxiv.org/abs/1312.5602v1),,,
,97,Proximal Policy Optimization Algorithms,,2017-07-20,John Schulman et al,[Link](http://arxiv.org/abs/1707.06347v2),,,
,98,Rainbow: Combining Improvements in Deep Reinforcement Learning,AAAI? 2018,2017-10-06,Matteo Hessel et al,[Link](http://arxiv.org/abs/1710.02298v1),,,
,99,Time Limits in Reinforcement Learning,CVPR 2018,2017-12-01,Fabio Pardo et al,[Link](http://arxiv.org/abs/1712.00378v4),,,
,100,Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research,NAACL 2018,2018-02-26,Matthias Plappert et al,[Link](http://arxiv.org/abs/1802.09464v2),,,
,101,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,,2020-04-08,Aravind Srinivas et al,[Link](http://arxiv.org/abs/2004.04136v4),,,
,102,Goal Density based Hindsight Experience Prioritization for Multi Goal Robot Manipulation Reinforcement Learning,ICLR 2021,2020-09-04,Yingyi Kuang et al,[Link](https://ieeexplore.ieee.org/document/9223473),,,
,103,The Role of Tactile Sensing in Learning and Deploying Grasp Refinement Algorithms,,2021-09-23,Alexander Koenig et al,[Link](http://arxiv.org/abs/2109.11234v2),,,
,104,The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks,,2021-10-12,Rahim Entezari et al,[Link](http://arxiv.org/abs/2110.06296v2),,,
,105,SoMoGym: A Toolkit for Developing and Evaluating Controllers and Reinforcement Learning Algorithms for Soft Robots,IEEE Robotics and Automation Letters 2022,2022-01-01,Moritz A. Graule et al,[Link](https://ieeexplore.ieee.org/document/9707663),,,
,106,Learning-Based Slip Detection for Robotic Fruit Grasping and Manipulation under Leaf Interference,NeurIPS 2022,2022-06-01,Hongyu Zhou et al,[Link](https://www.mdpi.com/1424-8220/22/15/5483),,,
,107,Augmenting Vision-Based Grasp Plans for Soft Robotic Grippers using Reinforcement Learning,NeurIPS 2022,2022-08-24,Vighnesh Vatsal et al,[Link](https://ieeexplore.ieee.org/document/9926580),,,
,108,DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning,ICRA? 2023,2023-01-25,I Made Aswin Nahrendra et al,[Link](http://arxiv.org/abs/2301.10602v2),,,
"<a id=""#security"">Security</a>",109,Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,IEEE Symposium on Security and Privacy (SP) 2019,2019-01-01,Bolun Wang et al,[Link](https://www.computer.org/csdl/proceedings-article/sp/2019/666000a707/1dlwir1mwFi),,,
,110,STRIP: A Defence Against Trojan Attacks on Deep Neural Networks,ICLR 2020,2019-02-18,Yansong Gao et al,[Link](http://arxiv.org/abs/1902.06531v2),,,
,111,BadNets: Evaluating Backdooring Attacks on Deep Neural Networks,NeurIPS 2019,2019-04-11,Tianyu Gu et al,[Link](https://ieeexplore.ieee.org/document/8685687),,,
,112,Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks,OpenAI,2019-05-24,Yuanshun Yao et al,[Link](http://arxiv.org/abs/1905.10447v1),,,
,113,Weight Poisoning Attacks on Pre-trained Models,ACL? 2020,2020-04-14,Keita Kurita et al,[Link](http://arxiv.org/abs/2004.06660v1),,,
,114,BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,ACSAC 2021,2020-06-01,Xiaoyi Chen et al,[Link](http://arxiv.org/abs/2006.01043v2),,,
,115,Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review,IEEE 2021,2020-07-21,Yansong Gao et al,[Link](http://arxiv.org/abs/2007.10760v3),,,
,116,Trojaning Language Models for Fun and Profit,NeurIPS 2021,2020-08-01,Xinyang Zhang et al,[Link](http://arxiv.org/abs/2008.00312v2),,,
,117,T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification,,2021-03-07,Ahmadreza Azizi et al,[Link](http://arxiv.org/abs/2103.04264v2),,,
,118,Hidden Backdoors in Human-Centric Language Models,CCS 2021,2021-01-01,Shaofeng Li et al,[Link](http://arxiv.org/abs/2105.00164),,,
,119,Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation,USENIX Security 2022,2022-01-01,Xudong Pan et al,[Link](https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden),,,
"<a id=""#computer-vision"">Computer Vision</a>",120,Auto-Encoding Variational Bayes,,2013-12-20,Diederik P Kingma et al,[Link](http://arxiv.org/abs/1312.6114v11),,,
,121,Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,,2015-02-10,Kelvin Xu et al,[Link](http://arxiv.org/abs/1502.03044v3),,
,122,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,,2016-06-12,Xi Chen et al,[Link](http://arxiv.org/abs/1606.03657v1),,,
,123,The Perception-Distortion Tradeoff,CVPR? 2018,2017-11-16,Yochai Blau et al,[Link](http://arxiv.org/abs/1711.06077v4),,,
,124,Do CIFAR-10 Classifiers Generalize to CIFAR-10?,,2018-06-01,Benjamin Recht et al,[Link](http://arxiv.org/abs/1806.00451v1),,,
,125,Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks,ICRA? 2019,2018-10-24,Michelle A. Lee et al,[Link](http://arxiv.org/abs/1810.10191v2),,,
,126,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,CVPR 2021,2020-10-22,Alexey Dosovitskiy et al,[Link](http://arxiv.org/abs/2010.11929v2),,,
,127,Training data-efficient image transformers & distillation through attention,ACL 2021,2020-12-23,Hugo Touvron et al,[Link](http://arxiv.org/abs/2012.12877v2),,,
